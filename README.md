# ğŸ›’ AmazonSentiment-BERTLSTM  
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)  
[![NLP](https://img.shields.io/badge/NLP-Sentiment%20Analysis-green)](https://en.wikipedia.org/wiki/Sentiment_analysis)  
[![Models](https://img.shields.io/badge/Models-LSTM%20%7C%20BERT%20%7C%20BitNet-orange)](https://huggingface.co/)  
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> Benchmarking LSTM, BERT, and BitLinear BERT for large-scale sentiment analysis on Amazon product reviews.

---

## ğŸ“˜ Overview

This project compares **three deep learning models** â€” **LSTM**, **BERT**, and **BitLinear BERT** â€” for sentiment analysis on the **Massive Amazon Reviews dataset (14M reviews)**.  
It highlights the trade-offs between accuracy, efficiency, and resource usage, exploring how quantized transformer layers can maintain performance while improving computational efficiency.

> **Goal:** Evaluate performance, robustness, and scalability of modern NLP architectures for large-scale sentiment classification.

---

## âš™ï¸ Installation & Run

### ğŸ§© 1. Clone the Repository
```bash
git clone https://github.com/HEsiyun/AmazonSentiment-BERTLSTM.git
cd AmazonSentiment-BERTLSTM
```

### ğŸ§  2. Create and Activate Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate   # For macOS/Linux
venv\Scripts\activate      # For Windows
```

### ğŸ“¦ 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### ğŸš€ 4. Run the Models
#### Run LSTM model
```bash
python lstm_model.py
```

#### Run BERT fine-tuning
```bash
python bert_model.py
```

#### Run BitLinear BERT
```bash
python bitlinear_bert.py
```

### ğŸ“Š 5. Visualize Results
```bash
python visualize.py
```
Generates confusion matrices, ROC curves, and performance summaries under `results/`.

> ğŸ’¡ Tip: If running on limited hardware, reduce batch size (e.g. 8â€“16) and set `max_length` to 100 in the tokenizer.

---

## ğŸ§  Models Compared

### 1ï¸âƒ£ LSTM
- Implemented with **TensorFlow/Keras**.  
- Handles sequential dependencies efficiently using embedding + LSTM layers.  
- Three variants were tested:
  - **Single-layer LSTM**  
  - **Multi-layer LSTM**  
  - **Bidirectional LSTM**  
- Best performance from **Bidirectional LSTM (input dim 10,000, dropout 0.2, lr=5e-5)** achieved:
  - **Accuracy:** 0.84
  - **Precision:** 0.83
  - **Recall:** 0.85
  - **AUC:** 0.92

> LSTM remains strong for sequential data but is sensitive to hyperparameters and batch sizes.

### 2ï¸âƒ£ BERT
- Based on **Hugging Faceâ€™s bert-base-cased**.
- Trained from scratch with a **custom classification head** for binary sentiment prediction.  
- Uses **AdamW optimizer** and **early stopping** to prevent overfitting.  
- Token length capped at 150 tokens for efficiency.
- Best configuration (batch=32, lr=3e-5, 3 epochs):
  - **Accuracy:** 0.9068
  - **Precision:** 0.91
  - **Recall:** 0.91
  - **AUC:** 0.97

> BERT achieved the **highest performance** across all metrics, proving robust to hyperparameter variation.

### 3ï¸âƒ£ BitLinear BERT (BitNet Quantized)
- Applies **1-bit quantization** via **BitLinear layers** to BERTâ€™s dense and attention components.  
- Reduces computational and memory overhead without major accuracy loss.  
- Best configuration (batch=64, lr=2e-5, 3 epochs):
  - **Accuracy:** 0.836
  - **Precision:** 0.84
  - **Recall:** 0.84
  - **AUC:** 0.92

> A **resource-efficient** BERT variant suitable for constrained environments â€” trades a small accuracy drop for major efficiency gains.

---

## ğŸ“Š Results Summary

| Model | Accuracy | Precision | Recall | AUC | Notes |
|:------|:----------|:-----------|:--------|:-----|:------|
| **BERT** | **0.9068** | **0.91** | **0.91** | **0.97** | Best overall performer |
| **LSTM (Bi)** | 0.84 | 0.83 | 0.85 | 0.92 | Strong sequential model |
| **BitLinear BERT** | 0.836 | 0.84 | 0.84 | 0.92 | Efficient quantized variant |

> BERT consistently achieved the highest accuracy and AUC, while BitLinear BERT offered a promising trade-off between efficiency and performance.

---

## ğŸ§© Dataset

- **Source:** [Massive Amazon Reviews Collection (14M lines)](https://www.kaggle.com/datasets/zakariaolamine/massive-amazon-reviews-collection-14m-lines)  
- **Preprocessing Steps:**
  - Random sampling (0.5% of data â‰ˆ 70k reviews).  
  - Cleaning: removed HTML tags, URLs, and non-alphabetic tokens.  
  - Undersampling for class balance (â‰ˆ21k positive, 21k negative).  
  - Sequence padding/truncation to 100â€“150 tokens.  
- **Visualizations:**
  - Class distributions before/after balancing.  
  - Word clouds pre/post cleaning.  
  - Sentence length histogram and vocabulary coverage plots.

---

## ğŸ§° Tech Stack

- **Python 3.10**  
- **TensorFlow / Keras** â€“ LSTM Implementation  
- **PyTorch + Hugging Face Transformers** â€“ BERT & BitLinear BERT  
- **BitNet Library (Kye Gomez)** â€“ 1-bit quantization layer replacement  
- **Pandas / Matplotlib / WordCloud** â€“ Data visualization and preprocessing  

---

## ğŸ§¾ Project Structure

```
AmazonSentiment-BERTLSTM/
â”œâ”€â”€ data/                          # Preprocessed samples from Kaggle dataset
â”œâ”€â”€ preprocessing.py               # Cleaning and balancing scripts
â”œâ”€â”€ lstm_model.py                  # LSTM implementation
â”œâ”€â”€ bert_model.py                  # BERT fine-tuning pipeline
â”œâ”€â”€ bitlinear_bert.py              # BitNet quantized BERT model
â”œâ”€â”€ train_utils.py                 # Training and evaluation functions
â”œâ”€â”€ visualize.py                   # Word clouds, confusion matrices, ROC plots
â””â”€â”€ report/                        # Final report and appendix
```

---

## ğŸ“ Author

**Siyun He**  
Khoury College of Computer Sciences, Northeastern University  
ğŸ“§ he.siyun@northeastern.edu  
ğŸŒ [GitHub: HEsiyun](https://github.com/HEsiyun)

---

## ğŸ’¬ Acknowledgments

- **Professor Uzair Ahmad** â€“ Course Instructor, CS6120 Natural Language Processing  
- **Venelin Valkov** â€“ BERT fine-tuning tutorial  
- **Jason Brownlee** â€“ LSTM sequence classification guide  
- **Kye Gomez** â€“ Creator of BitNet quantization library  

---

â­ **If this project helps you, please give it a star!**

